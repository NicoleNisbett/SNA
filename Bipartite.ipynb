{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script to join data sources to create bipartite user,topic network edge lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic import BERTopic\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle    \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction import text\n",
    "import re\n",
    "from collections import Counter\n",
    "import preprocessor as p\n",
    "import glob\n",
    "\n",
    "#from wordcloud import WordCloud\n",
    "#import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "loadPath = \"/Users/ipinni/Library/CloudStorage/OneDrive-UniversityofLeeds/UKRI_Tweet_Data/completed/\"\n",
    "loadPath2 = \"/Users/ipinni/Library/CloudStorage/OneDrive-UniversityofLeeds/UKRI_Tweet_Data/tweets/COP/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loads data from COPs with single files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(version):\n",
    "\n",
    "    with open(loadPath + version + \"/\" + version + \"topics.list\" ,'rb') as config_list_file:   \n",
    "        topics = pickle.load(config_list_file)\n",
    "\n",
    "    with open(loadPath + version + \"/\" + version + \"docs.list\", 'rb') as docs_list_file:   \n",
    "        docs = pickle.load(docs_list_file)\n",
    "\n",
    "    results = pd.DataFrame({\"text\": docs, \"topic\": topics})\n",
    "\n",
    "    #tweets = pd.read_csv(loadPath + version + \"/\" + version + \"CleanTweets.csv\")\n",
    "\n",
    "    users = pd.read_csv(loadPath2 + \"tweets\" + version + \".csv\")\n",
    "    users = users[users.sourcetweet_lang == 'en']\n",
    "    users = users[[\"user_username\", \"sourcetweet_id\", \"sourcetweet_text\"]]\n",
    "\n",
    "    return(results, users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "COP20results, COP20users = get_data(version = \"COP20\")\n",
    "COP22results, COP22users = get_data(version = \"COP22\")\n",
    "COP23results, COP23users = get_data(version = \"COP23\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loads data from COPs with multiple files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data2(version):\n",
    "\n",
    "    with open(loadPath + version + \"/\" + version + \"topics.list\" ,'rb') as config_list_file:   \n",
    "        topics = pickle.load(config_list_file)\n",
    "\n",
    "    with open(loadPath + version + \"/\" + version + \"docs.list\", 'rb') as docs_list_file:   \n",
    "        docs = pickle.load(docs_list_file)\n",
    "\n",
    "    results = pd.DataFrame({\"text\": docs, \"topic\": topics})\n",
    "\n",
    "    #get the COPs with multiple files and concat to one\n",
    "    filelist=[]\n",
    "    for files in glob.glob(loadPath2 + \"tweets\" + version + \"*\"):\n",
    "        filelist.append(files)\n",
    "    \n",
    "    userslist = []\n",
    "    for i in filelist:\n",
    "        df = pd.read_csv(i)\n",
    "        userslist.append(df)\n",
    "\n",
    "    users = pd.concat(userslist, axis=0, ignore_index=True)\n",
    "    users = users[users.sourcetweet_lang == 'en']\n",
    "    users = users[[\"user_username\", \"sourcetweet_id\", \"sourcetweet_text\"]]\n",
    "\n",
    "    return(results, users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COP21results, COP21users = get_data2(version = \"COP21\")\n",
    "COP24results, COP24users = get_data2(version = \"COP24\")\n",
    "COP25results, COP25users = get_data2(version = \"COP25\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "COP26results, COP26users = get_data2(version = \"COP26\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean tweets, match tweet author to tweet to topic as determined by BERTopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.set_options(p.OPT.URL, p.OPT.RESERVED)\n",
    "def merge_dfs(results, users,version, frames):\n",
    "\n",
    "  cleanusers = []\n",
    "  for i in range(len(users)):\n",
    "    cleanusers.append(p.clean(users.sourcetweet_text.iloc[i]))\n",
    "  users[\"sourcetweet_text\"] = cleanusers\n",
    "\n",
    "  #combine the tweets and results to produce [tweet id, text, like, retweet, topic] df\n",
    "  merged1 = users.merge(results.drop_duplicates(subset=['text']), left_on=\"sourcetweet_text\", right_on= \"text\", how = \"left\")\n",
    "  bipartite = merged1[[\"user_username\", \"topic\"]]\n",
    "  bipartite.columns = [\"Source\", \"Target\"]\n",
    "  bipartite = bipartite.dropna()\n",
    "  \n",
    "  bipartite = bipartite[bipartite[\"Target\"].isin(frames)]\n",
    "\n",
    "  \n",
    "  #merged2 = merged1.merge(tweets.drop_duplicates(subset=[\"tweet_id\"]), left_on = \"sourcetweet_id\", right_on = \"tweet_id\", how = \"left\")\n",
    "\n",
    "  bipartite.to_csv(\"Merged/\" + version + \"Merged.csv\")\n",
    "\n",
    "  #return(bipartite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "COP20_frames = [3,4,5,7,14,22,26,40,41,42,44]\n",
    "merge_dfs(COP20results, COP20users, \"COP20\", COP20_frames)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "COP21_frames = [3,4,9,14,15,19,29]\n",
    "merge_dfs(COP21results, COP21users, \"COP21\", COP21_frames)\n",
    "\n",
    "COP22_frames = [13,15,16,17,18,28]\n",
    "merge_dfs(COP22results, COP22users, \"COP22\", COP22_frames)\n",
    "\n",
    "COP23_frames = [4,13,17,18,31]\n",
    "merge_dfs(COP23results, COP23users, \"COP23\", COP23_frames)\n",
    "\n",
    "COP24_frames = [2,8,17,20,21,26,35,43,44,46,48,51,55,56]\n",
    "merge_dfs(COP24results, COP24users, \"COP24\", COP24_frames)\n",
    "\n",
    "COP25_frames = [3,4,7,8,16,18,26,32,35,43,65]\n",
    "merge_dfs(COP25results, COP25users, \"COP25\", COP25_frames)                        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using smaller subset of frames\n",
    "COP26_frames = [5,10,23,27,37]\n",
    "merge_dfs(COP26results, COP26users, \"COP26\", COP26_frames) #this one takes ages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NetworkX bipartite projections takes too much memory for anything other than COP20 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from networkx.algorithms import bipartite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_projections(merge):\n",
    "    g = nx.Graph()\n",
    "    user = []\n",
    "    topic = []\n",
    "\n",
    "    for i in range(len(merge)):\n",
    "        u = merge.user_username.iloc[i]\n",
    "        t = merge.topic.iloc[i]\n",
    "        g.add_edge(u, t)\n",
    "        user.append(u)\n",
    "        topic.append(t)\n",
    "\n",
    "    Ngraph_user = bipartite.weighted_projected_graph(g, user)\n",
    "\n",
    "    return(Ngraph_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_projections2(merge, version):\n",
    "    g = nx.Graph()\n",
    "    user = []\n",
    "    topic = []\n",
    "\n",
    "    with open(merge, \"r\") as f:\n",
    "        f.readline()\n",
    "        for l in f:\n",
    "            l = l.rstrip().split(\",\")\n",
    "            u = str(l[1])\n",
    "            t = str(l[2])\n",
    "            g.add_edge(u, t)\n",
    "            user.append(u)\n",
    "            topic.append(t)\n",
    "\n",
    "    Ngraph_user = bipartite.weighted_projected_graph(g, user)\n",
    "    nx.write_edgelist(Ngraph_user, \"Projections/\" + version +\"Projections.csv\",  delimiter = \",\")\n",
    "\n",
    "    return(Ngraph_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = \"COP20\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COP20proj = get_projections2(\"Merged/\" + version + \"Merged.csv\", version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.write_edgelist(COP20proj, \"Projections/\" + version +\"Projections.csv\", delimiter = \",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projs = pd.read_csv(\"Projections/COP20Projections.csv\",sep= \" \" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from networkx.algorithms import bipartite\n",
    "g = nx.Graph()\n",
    "user = []\n",
    "topic = []\n",
    "\n",
    "with open(\"Trimester Bipartite/SepDec17.csv\", \"r\") as f:\n",
    "    f.readline()\n",
    "    for l in f:\n",
    "        l = l.rstrip().split(\",\")\n",
    "        u = str(l[0])\n",
    "        t = str(l[3])\n",
    "        g.add_edge(u, t)\n",
    "        user.append(u)\n",
    "        topic.append(t)\n",
    "\n",
    "\n",
    "Ngraph_user = bipartite.weighted_projected_graph(g, user)\n",
    "nx.write_edgelist(Ngraph_user, \"Proj_SepDec17.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_most_similar_topics(cG: nx.Graph):\n",
    "    \"\"\"\n",
    "    Find the crimes that are most similar to other crimes.\n",
    "    \"\"\"\n",
    "    dcs = pd.Series(nx.degree_centrality(cG))\n",
    "    return dcs.sort_values(ascending=False).head(20)\n",
    "\n",
    "find_most_similar_topics(COP20proj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.10.5 ('gt')' requires ipykernel package.\n",
      "Run the following command to install 'ipykernel' into the Python environment. \n",
      "Command: 'conda install -n gt ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.11 ('bertopic_env2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fe4d8685886068fc374377cb0ce03965843ae3df30816ebe900dd55a8a561255"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
