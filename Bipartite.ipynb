{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script to join data sources to create bipartite user,topic network edge lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic import BERTopic\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle    \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction import text\n",
    "import re\n",
    "from collections import Counter\n",
    "import preprocessor as p\n",
    "import glob\n",
    "\n",
    "#from wordcloud import WordCloud\n",
    "#import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "loadPath = \"/Users/ipinni/Library/CloudStorage/OneDrive-UniversityofLeeds/UKRI_Tweet_Data/completed/\"\n",
    "loadPath2 = \"/Users/ipinni/Library/CloudStorage/OneDrive-UniversityofLeeds/UKRI_Tweet_Data/tweets/COP/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loads data from COPs with single files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(version):\n",
    "\n",
    "    with open(loadPath + version + \"/\" + version + \"topics.list\" ,'rb') as config_list_file:   \n",
    "        topics = pickle.load(config_list_file)\n",
    "\n",
    "    with open(loadPath + version + \"/\" + version + \"docs.list\", 'rb') as docs_list_file:   \n",
    "        docs = pickle.load(docs_list_file)\n",
    "\n",
    "    results = pd.DataFrame({\"text\": docs, \"topic\": topics})\n",
    "\n",
    "    #tweets = pd.read_csv(loadPath + version + \"/\" + version + \"CleanTweets.csv\")\n",
    "\n",
    "    users = pd.read_csv(loadPath2 + \"tweets\" + version + \".csv\")\n",
    "    users = users[users.sourcetweet_lang == 'en']\n",
    "    users = users[[\"user_username\", \"sourcetweet_id\", \"sourcetweet_text\"]]\n",
    "\n",
    "    return(results, users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "COP20results, COP20users = get_data(version = \"COP20\")\n",
    "COP22results, COP22users = get_data(version = \"COP22\")\n",
    "COP23results, COP23users = get_data(version = \"COP23\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loads data from COPs with multiple files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data2(version):\n",
    "\n",
    "    with open(loadPath + version + \"/\" + version + \"topics.list\" ,'rb') as config_list_file:   \n",
    "        topics = pickle.load(config_list_file)\n",
    "\n",
    "    with open(loadPath + version + \"/\" + version + \"docs.list\", 'rb') as docs_list_file:   \n",
    "        docs = pickle.load(docs_list_file)\n",
    "\n",
    "    results = pd.DataFrame({\"text\": docs, \"topic\": topics})\n",
    "\n",
    "    #get the COPs with multiple files and concat to one\n",
    "    filelist=[]\n",
    "    for files in glob.glob(loadPath2 + \"tweets\" + version + \"*\"):\n",
    "        filelist.append(files)\n",
    "    \n",
    "    userslist = []\n",
    "    for i in filelist:\n",
    "        df = pd.read_csv(i)\n",
    "        userslist.append(df)\n",
    "\n",
    "    users = pd.concat(userslist, axis=0, ignore_index=True)\n",
    "    users = users[users.sourcetweet_lang == 'en']\n",
    "    users = users[[\"user_username\", \"sourcetweet_id\", \"sourcetweet_text\"]]\n",
    "\n",
    "    return(results, users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ipinni/.conda/envs/bertopic_env2/lib/python3.7/site-packages/ipykernel_launcher.py:2: DtypeWarning: Columns (14,16) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  \n",
      "/Users/ipinni/.conda/envs/bertopic_env2/lib/python3.7/site-packages/ipykernel_launcher.py:2: DtypeWarning: Columns (12,13) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  \n",
      "/Users/ipinni/.conda/envs/bertopic_env2/lib/python3.7/site-packages/ipykernel_launcher.py:2: DtypeWarning: Columns (12,16) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  \n",
      "/Users/ipinni/.conda/envs/bertopic_env2/lib/python3.7/site-packages/ipykernel_launcher.py:2: DtypeWarning: Columns (12,17) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  \n",
      "/Users/ipinni/.conda/envs/bertopic_env2/lib/python3.7/site-packages/ipykernel_launcher.py:2: DtypeWarning: Columns (17,18) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  \n",
      "/Users/ipinni/.conda/envs/bertopic_env2/lib/python3.7/site-packages/ipykernel_launcher.py:2: DtypeWarning: Columns (10,15) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  \n",
      "/Users/ipinni/.conda/envs/bertopic_env2/lib/python3.7/site-packages/ipykernel_launcher.py:2: DtypeWarning: Columns (12,15) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "COP21results, COP21users = get_data2(version = \"COP21\")\n",
    "COP24results, COP24users = get_data2(version = \"COP24\")\n",
    "COP25results, COP25users = get_data2(version = \"COP25\")\n",
    "COP26results, COP26users = get_data2(version = \"COP26\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean tweets, match tweet author to tweet to topic as determined by BERTopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.set_options(p.OPT.URL, p.OPT.RESERVED)\n",
    "def merge_dfs(results, users,version):\n",
    "\n",
    "  cleanusers = []\n",
    "  for i in range(len(users)):\n",
    "    cleanusers.append(p.clean(users.sourcetweet_text.iloc[i]))\n",
    "  users[\"sourcetweet_text\"] = cleanusers\n",
    "\n",
    "  #combine the tweets and results to produce [tweet id, text, like, retweet, topic] df\n",
    "  merged1 = users.merge(results.drop_duplicates(subset=['text']), left_on=\"sourcetweet_text\", right_on= \"text\", how = \"left\")\n",
    "  bipartite = merged1[[\"user_username\", \"topic\"]]\n",
    "  bipartite.columns = [\"Source\", \"Target\"]\n",
    "  bipartite = bipartite.dropna()\n",
    "  \n",
    "  \n",
    "  #merged2 = merged1.merge(tweets.drop_duplicates(subset=[\"tweet_id\"]), left_on = \"sourcetweet_id\", right_on = \"tweet_id\", how = \"left\")\n",
    "\n",
    "  bipartite.to_csv(\"Merged/\" + version + \"Merged.csv\")\n",
    "\n",
    "  #return(bipartite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_dfs(COP20results, COP20users, \"COP20\")\n",
    "merge_dfs(COP21results, COP21users, \"COP21\")\n",
    "merge_dfs(COP22results, COP22users, \"COP22\")\n",
    "merge_dfs(COP23results, COP23users, \"COP23\")\n",
    "merge_dfs(COP24results, COP24users, \"COP24\")\n",
    "merge_dfs(COP25results, COP25users, \"COP25\")\n",
    "merge_dfs(COP26results, COP26users, \"COP26\") #this one takes ages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NetworkX bipartite projections takes too much memory for anything other than COP20 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from networkx.algorithms import bipartite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_projections(merge):\n",
    "    g = nx.Graph()\n",
    "    user = []\n",
    "    topic = []\n",
    "\n",
    "    for i in range(len(merge)):\n",
    "        u = merge.user_username.iloc[i]\n",
    "        t = merge.topic.iloc[i]\n",
    "        g.add_edge(u, t)\n",
    "        user.append(u)\n",
    "        topic.append(t)\n",
    "\n",
    "    Ngraph_user = bipartite.weighted_projected_graph(g, user)\n",
    "\n",
    "    return(Ngraph_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_projections2(merge, version):\n",
    "    g = nx.Graph()\n",
    "    user = []\n",
    "    topic = []\n",
    "\n",
    "    with open(merge, \"r\") as f:\n",
    "        f.readline()\n",
    "        for l in f:\n",
    "            l = l.rstrip().split(\",\")\n",
    "            u = str(l[1])\n",
    "            t = str(l[2])\n",
    "            g.add_edge(u, t)\n",
    "            user.append(u)\n",
    "            topic.append(t)\n",
    "\n",
    "    Ngraph_user = bipartite.weighted_projected_graph(g, user)\n",
    "    nx.write_edgelist(Ngraph_user, \"Projections/\" + version +\"Projections.csv\",  delimiter = \",\")\n",
    "\n",
    "    return(Ngraph_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = \"COP20\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COP20proj = get_projections2(\"Merged/\" + version + \"Merged.csv\", version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.write_edgelist(COP20proj, \"Projections/\" + version +\"Projections.csv\", delimiter = \",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projs = pd.read_csv(\"Projections/COP20Projections.csv\",sep= \" \" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from networkx.algorithms import bipartite\n",
    "g = nx.Graph()\n",
    "user = []\n",
    "topic = []\n",
    "\n",
    "with open(\"Trimester Bipartite/SepDec17.csv\", \"r\") as f:\n",
    "    f.readline()\n",
    "    for l in f:\n",
    "        l = l.rstrip().split(\",\")\n",
    "        u = str(l[0])\n",
    "        t = str(l[3])\n",
    "        g.add_edge(u, t)\n",
    "        user.append(u)\n",
    "        topic.append(t)\n",
    "\n",
    "\n",
    "Ngraph_user = bipartite.weighted_projected_graph(g, user)\n",
    "nx.write_edgelist(Ngraph_user, \"Proj_SepDec17.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_most_similar_topics(cG: nx.Graph):\n",
    "    \"\"\"\n",
    "    Find the crimes that are most similar to other crimes.\n",
    "    \"\"\"\n",
    "    dcs = pd.Series(nx.degree_centrality(cG))\n",
    "    return dcs.sort_values(ascending=False).head(20)\n",
    "\n",
    "find_most_similar_topics(COP20proj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.10.5 ('gt')' requires ipykernel package.\n",
      "Run the following command to install 'ipykernel' into the Python environment. \n",
      "Command: 'conda install -n gt ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('gt')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4df457cff4e7bbbaf3d2f1679f42b3a27b65d652a7f6a314c66c9d82ad6cd340"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
